{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 1: Multiple Linear Regression completed. Metrics and coefficients saved.\n",
      "MSE: 0.91492, RMSE: 0.95652, MAE: 0.75565, R2: 0.19822, MAPE: 14.57828%, Accuracy: 85.42172%\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"drying_time_dataset.csv\")  \n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"drying_time\"], errors=\"ignore\")\n",
    "y = df[\"drying_time\"]\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Train the multiple linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 5. Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 6. Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "accuracy = 100 - mape\n",
    "\n",
    "# 7. Save metrics \n",
    "results = pd.DataFrame({\n",
    "    \"Version\": [\"Version 1: Multiple Linear Regression\"],\n",
    "    \"MSE\": [mse],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2\": [r2],\n",
    "    \"MAPE (%)\": [mape],\n",
    "    \"Accuracy (%)\": [accuracy]\n",
    "})\n",
    "results.to_csv(\"drying_time_model_comparison.csv\", mode=\"a\", index=False, header=False)\n",
    "\n",
    "# 8. Save model coefficients to 'model_v1_coefficients.csv'\n",
    "coefficients = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": model.coef_\n",
    "})\n",
    "coefficients.to_csv(\"model_v1_coefficients.csv\", index=False)\n",
    "\n",
    "print(\"Version 1: Multiple Linear Regression completed. Metrics and coefficients saved.\")\n",
    "print(f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, \"\n",
    "      f\"R2: {r2:.5f}, MAPE: {mape:.5f}%, Accuracy: {accuracy:.5f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERSION 2: MLR-Skewness Correction (log transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewed Features (abs(skew) > 0.75): ['mc_initial', 'temperature', 'humidity']\n",
      "Applied log1p transform to 'mc_initial'.\n",
      "Applied log1p transform to 'temperature'.\n",
      "Applied log1p transform to 'humidity'.\n",
      "Version 2 (MLR-Skewness Correction) completed. Metrics and coefficients saved.\n",
      "Skewed features transformed: ['mc_initial', 'temperature', 'humidity']\n",
      "MSE: 0.92390, RMSE: 0.96120, MAE: 0.76423, R2: 0.19036, MAPE: 14.74842%, Accuracy: 85.25158%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv(\"drying_time_dataset.csv\")\n",
    "\n",
    "# 2. Identify and transform skewed features\n",
    "#    Compute skew and apply log1p for features with abs(skew) > 0.75 (example threshold).\n",
    "threshold = 0.75\n",
    "skew_vals = df.drop(columns=[\"drying_time\"], errors=\"ignore\").skew().sort_values(ascending=False)\n",
    "\n",
    "# Find columns to transform\n",
    "skewed_features = skew_vals[abs(skew_vals) > threshold].index.tolist()\n",
    "print(\"Skewed Features (abs(skew) > 0.75):\", skewed_features)\n",
    "\n",
    "# Apply log(1+x) transform for skewed features (only if the data is non-negative)\n",
    "for col in skewed_features:\n",
    "    # Ensure no negative values before log transform.\n",
    "    if (df[col] < 0).any():\n",
    "        print(f\"Warning: Column '{col}' has negative values; skipping log transform.\")\n",
    "    else:\n",
    "        df[col] = np.log1p(df[col])\n",
    "        print(f\"Applied log1p transform to '{col}'.\")\n",
    "\n",
    "# 3. Split features and target\n",
    "X = df.drop(columns=[\"drying_time\"], errors=\"ignore\")\n",
    "y = df[\"drying_time\"]\n",
    "\n",
    "# 4. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 5. Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 6. Train Multiple Linear Regression Model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 7. Make Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 8. Calculate Metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "accuracy = 100 - mape\n",
    "\n",
    "# 9. Save Metrics \n",
    "results = pd.DataFrame({\n",
    "    \"Version\": [\"Version 2: MLR- Skewness Correction (Log Transform)\"],\n",
    "    \"MSE\": [mse],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2\": [r2],\n",
    "    \"MAPE (%)\": [mape],\n",
    "    \"Accuracy (%)\": [accuracy]\n",
    "})\n",
    "results.to_csv(\"drying_time_model_comparison.csv\", mode=\"a\", index=False, header=False)\n",
    "\n",
    "# 10. Save Coefficients \n",
    "coefficients = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": model.coef_\n",
    "})\n",
    "coefficients.to_csv(\"model_v2_coefficients.csv\", index=False)\n",
    "\n",
    "print(\"Version 2 (MLR-Skewness Correction) completed. Metrics and coefficients saved.\")\n",
    "print(\"Skewed features transformed:\", skewed_features)\n",
    "print(f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, \"\n",
    "      f\"R2: {r2:.5f}, MAPE: {mape:.5f}%, Accuracy: {accuracy:.5f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 3: Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 3 (Random Forest Regression) completed. Metrics and importances saved.\n",
      "MSE: 0.04052, RMSE: 0.20129, MAE: 0.09348, R2: 0.96449, MAPE: 1.90728%, Accuracy: 98.09272%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv(\"drying_time_dataset.csv\")\n",
    "\n",
    "# 2. Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"drying_time\"], errors=\"ignore\")\n",
    "y = df[\"drying_time\"]\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Scale features \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Train Random Forest Regressor \n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Make Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 7. Calculate Metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "accuracy = 100 - mape\n",
    "\n",
    "# 8. Save Metrics \n",
    "results = pd.DataFrame({\n",
    "    \"Version\": [\"Version 3: Random Forest Regression\"],\n",
    "    \"MSE\": [mse],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2\": [r2],\n",
    "    \"MAPE (%)\": [mape],\n",
    "    \"Accuracy (%)\": [accuracy]\n",
    "})\n",
    "results.to_csv(\"drying_time_model_comparison.csv\", mode=\"a\", index=False, header=False)\n",
    "\n",
    "# 9. Save Feature Importances\n",
    "importances = model.feature_importances_\n",
    "importances_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "importances_df.to_csv(\"model_v3_feature_importances.csv\", index=False)\n",
    "\n",
    "joblib.dump(model, 'drying_time_rf_model.joblib')\n",
    "joblib.dump(scaler, 'drying_time_rf_scaler.joblib')\n",
    "\n",
    "print(\"Version 3 (Random Forest Regression) completed. Metrics and importances saved.\")\n",
    "print(f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, \"\n",
    "      f\"R2: {r2:.5f}, MAPE: {mape:.5f}%, Accuracy: {accuracy:.5f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 4: Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 4 (Gradient Boosting Regression) completed. Metrics and importances saved.\n",
      "MSE: 0.04873, RMSE: 0.22075, MAE: 0.14996, R2: 0.95729, MAPE: 2.81998%, Accuracy: 97.18002%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv(\"drying_time_dataset.csv\")\n",
    "\n",
    "# 2. Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"drying_time\"], errors=\"ignore\")\n",
    "y = df[\"drying_time\"]\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Scale features (for consistency with previous versions)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Train Gradient Boosting Regressor\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Make Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 7. Calculate Metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "accuracy = 100 - mape\n",
    "\n",
    "# 8. Save Metrics \n",
    "results = pd.DataFrame({\n",
    "    \"Version\": [\"Version 4: Gradient Boosting Regression\"],\n",
    "    \"MSE\": [mse],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2\": [r2],\n",
    "    \"MAPE (%)\": [mape],\n",
    "    \"Accuracy (%)\": [accuracy]\n",
    "})\n",
    "results.to_csv(\"drying_time_model_comparison.csv\", mode=\"a\", index=False, header=False)\n",
    "\n",
    "# 9.Feature Importances\n",
    "importances = model.feature_importances_\n",
    "importances_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "importances_df.to_csv(\"model_v4_feature_importances.csv\", index=False)\n",
    "\n",
    "print(\"Version 4 (Gradient Boosting Regression) completed. Metrics and importances saved.\")\n",
    "print(f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, \"\n",
    "      f\"R2: {r2:.5f}, MAPE: {mape:.5f}%, Accuracy: {accuracy:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 5: XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 5 (XGBoost Regression) completed. Metrics and importances saved.\n",
      "MSE: 0.06150, RMSE: 0.24799, MAE: 0.17329, R2: 0.94611, MAPE: 3.21697%, Accuracy: 96.78303%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv(\"drying_time_dataset.csv\")\n",
    "\n",
    "# 2. Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"drying_time\"], errors=\"ignore\")\n",
    "y = df[\"drying_time\"]\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Scale features \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Train XGBoost Regressor\n",
    "model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Make Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 7. Calculate Metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "accuracy = 100 - mape\n",
    "\n",
    "# 8. Save Metrics \n",
    "results = pd.DataFrame({\n",
    "    \"Version\": [\"Version 5: XGBoost Regression\"],\n",
    "    \"MSE\": [mse],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2\": [r2],\n",
    "    \"MAPE (%)\": [mape],\n",
    "    \"Accuracy (%)\": [accuracy]\n",
    "})\n",
    "results.to_csv(\"drying_time_model_comparison.csv\", mode=\"a\", index=False, header=False)\n",
    "\n",
    "# 9. Feature Importances\n",
    "importances = model.feature_importances_\n",
    "importances_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "importances_df.to_csv(\"model_v5_feature_importances.csv\", index=False)\n",
    "\n",
    "print(\"Version 5 (XGBoost Regression) completed. Metrics and importances saved.\")\n",
    "print(f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, \"\n",
    "      f\"R2: {r2:.5f}, MAPE: {mape:.5f}%, Accuracy: {accuracy:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 6: Multilayer Perceptron Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 6 (MLP Regression) completed. Metrics saved.\n",
      "MSE: 0.32014, RMSE: 0.56581, MAE: 0.34729, R2: 0.71945, MAPE: 6.18545%, Accuracy: 93.81455%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Desktop\\Thesis\\1.Paddyrice\\Machine Learning\\ML\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv(\"drying_time_dataset.csv\")\n",
    "\n",
    "# 2. Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"drying_time\"], errors=\"ignore\")\n",
    "y = df[\"drying_time\"]\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Configure and train MLP Regressor \n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    learning_rate='constant',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 7. Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "accuracy = 100 - mape\n",
    "\n",
    "# 8. Save Metrics \n",
    "results = pd.DataFrame({\n",
    "    \"Version\": [\"Version 6: Multilayer Perceptron Regression\"],\n",
    "    \"MSE\": [mse],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2\": [r2],\n",
    "    \"MAPE (%)\": [mape],\n",
    "    \"Accuracy (%)\": [accuracy]\n",
    "})\n",
    "results.to_csv(\"drying_time_model_comparison.csv\", mode=\"a\", index=False, header=False)\n",
    "\n",
    "# 9. Display / Print\n",
    "print(\"Version 6 (MLP Regression) completed. Metrics saved.\")\n",
    "print(f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, \"\n",
    "      f\"R2: {r2:.5f}, MAPE: {mape:.5f}%, Accuracy: {accuracy:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 7: MLP Regression (Modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 7 (MLP Modified) completed. Metrics saved.\n",
      "MSE: 0.43004, RMSE: 0.65578, MAE: 0.43425, R2: 0.62314, MAPE: 7.85214%, Accuracy: 92.14786%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv(\"drying_time_dataset.csv\")\n",
    "\n",
    "# 2. Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"drying_time\"], errors=\"ignore\")\n",
    "y = df[\"drying_time\"]\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Configure and train MLP Regressor\n",
    "#    - deeper hidden layers (200, 100, 50)\n",
    "#    - early_stopping=True to stop when validation stops improving\n",
    "#    - alpha=0.001 for stronger regularization\n",
    "#    - learning_rate_init=0.001 for finer updates\n",
    "#    - max_iter=2000 for enough training epochs\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(200, 100, 50),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    learning_rate='constant',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=2000,\n",
    "    early_stopping=True,      # splits off a fraction of training data for validation\n",
    "    validation_fraction=0.1,  # 10% of the training set is used as validation\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 7. Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "accuracy = 100 - mape\n",
    "\n",
    "# 8. Save Metrics \n",
    "results = pd.DataFrame({\n",
    "    \"Version\": [\"Version 7: MLP (Modified)\"],\n",
    "    \"MSE\": [mse],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2\": [r2],\n",
    "    \"MAPE (%)\": [mape],\n",
    "    \"Accuracy (%)\": [accuracy]\n",
    "})\n",
    "results.to_csv(\"drying_time_model_comparison.csv\", mode=\"a\", index=False, header=False)\n",
    "\n",
    "# 9. Display / Print\n",
    "print(\"Version 7 (MLP Modified) completed. Metrics saved.\")\n",
    "print(f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, \"\n",
    "      f\"R2: {r2:.5f}, MAPE: {mape:.5f}%, Accuracy: {accuracy:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 8: MLP (Adaptive LR + Deeper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 8 (MLP - Adaptive LR + Deeper) completed. Metrics saved.\n",
      "MSE: 0.34348, RMSE: 0.58607, MAE: 0.39578, R2: 0.69900, MAPE: 7.01006%, Accuracy: 92.98994%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv(\"drying_time_dataset.csv\")\n",
    "\n",
    "# 2. Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"drying_time\"], errors=\"ignore\")\n",
    "y = df[\"drying_time\"]\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Configure and train MLP Regressor\n",
    "#    - Hidden layers: (300, 200, 100, 50)\n",
    "#    - learning_rate='adaptive'\n",
    "#    - smaller learning_rate_init=0.0005\n",
    "#    - alpha=0.001 still\n",
    "#    - max_iter=3000\n",
    "#    - early_stopping=True\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(300, 200, 100, 50),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.0005,\n",
    "    max_iter=3000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 7. Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "accuracy = 100 - mape\n",
    "\n",
    "# 8. Save Metrics \n",
    "results = pd.DataFrame({\n",
    "    \"Version\": [\"Version 8: MLP (Adaptive LR + Deeper)\"],\n",
    "    \"MSE\": [mse],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2\": [r2],\n",
    "    \"MAPE (%)\": [mape],\n",
    "    \"Accuracy (%)\": [accuracy]\n",
    "})\n",
    "results.to_csv(\"drying_time_model_comparison.csv\", mode=\"a\", index=False, header=False)\n",
    "\n",
    "# 9. Print out results\n",
    "print(\"Version 8 (MLP - Adaptive LR + Deeper) completed. Metrics saved.\")\n",
    "print(f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, \"\n",
    "      f\"R2: {r2:.5f}, MAPE: {mape:.5f}%, Accuracy: {accuracy:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 9: MLP (Increased Alpha + Wider Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 9 (MLP - Increased Alpha + Wider Layers) completed. Metrics saved.\n",
      "MSE: 0.36348, RMSE: 0.60290, MAE: 0.39569, R2: 0.68147, MAPE: 7.07143%, Accuracy: 92.92857%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv(\"drying_time_dataset.csv\")\n",
    "\n",
    "# 2. Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"drying_time\"], errors=\"ignore\")\n",
    "y = df[\"drying_time\"]\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Configure and train MLP Regressor\n",
    "#    - Increased alpha (0.01) for stronger regularization\n",
    "#    - Wider hidden layers (512, 256, 128, 64)\n",
    "#    - learning_rate='adaptive'\n",
    "#    - max_iter=3000 for full convergence\n",
    "#    - early_stopping=True\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(512, 256, 128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.01,  # Increased alpha (stronger regularization)\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.0005,\n",
    "    max_iter=3000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 7. Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "accuracy = 100 - mape\n",
    "\n",
    "# 8. Save Metrics (append) to 'removed_model_comparison.csv'\n",
    "results = pd.DataFrame({\n",
    "    \"Version\": [\"Version 9: MLP (Increased Alpha + Wider Layers)\"],\n",
    "    \"MSE\": [mse],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2\": [r2],\n",
    "    \"MAPE (%)\": [mape],\n",
    "    \"Accuracy (%)\": [accuracy]\n",
    "})\n",
    "results.to_csv(\"drying_time_model_comparison.csv\", mode=\"a\", index=False, header=False)\n",
    "\n",
    "# 9. Print results\n",
    "print(\"Version 9 (MLP - Increased Alpha + Wider Layers) completed. Metrics saved.\")\n",
    "print(f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, \"\n",
    "      f\"R2: {r2:.5f}, MAPE: {mape:.5f}%, Accuracy: {accuracy:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 10: MLP (Higher Alpha + Dropout Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 10 (MLP - Higher Alpha + Dropout Simulation) completed. Metrics saved.\n",
      "MSE: 0.39039, RMSE: 0.62481, MAE: 0.43238, R2: 0.65789, MAPE: 7.79063%, Accuracy: 92.20937%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv(\"drying_time_dataset.csv\")\n",
    "\n",
    "# 2. Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"drying_time\"], errors=\"ignore\")\n",
    "y = df[\"drying_time\"]\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Configure and train MLP Regressor\n",
    "#    - Very high alpha (0.05) to simulate dropout-like behavior\n",
    "#    - Medium-depth architecture (256, 128, 64, 32)\n",
    "#    - learning_rate='adaptive'\n",
    "#    - max_iter=3000 for deep learning stability\n",
    "#    - early_stopping=True\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(256, 128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.05,  # Strong regularization to simulate dropout\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.0005,\n",
    "    max_iter=3000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 7. Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "accuracy = 100 - mape\n",
    "\n",
    "# 8. Save Metrics \n",
    "results = pd.DataFrame({\n",
    "    \"Version\": [\"Version 10: MLP (Higher Alpha + Dropout Simulation)\"],\n",
    "    \"MSE\": [mse],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2\": [r2],\n",
    "    \"MAPE (%)\": [mape],\n",
    "    \"Accuracy (%)\": [accuracy]\n",
    "})\n",
    "results.to_csv(\"drying_time_model_comparison.csv\", mode=\"a\", index=False, header=False)\n",
    "\n",
    "# 9. Print results\n",
    "print(\"Version 10 (MLP - Higher Alpha + Dropout Simulation) completed. Metrics saved.\")\n",
    "print(f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, \"\n",
    "      f\"R2: {r2:.5f}, MAPE: {mape:.5f}%, Accuracy: {accuracy:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 11: MLP (Batch Normalization + Increased Iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 11 (MLP - Batch Normalization + Increased Iterations) completed. Metrics saved.\n",
      "MSE: 0.38872, RMSE: 0.62347, MAE: 0.40831, R2: 0.65935, MAPE: 7.18643%, Accuracy: 92.81357%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv(\"drying_time_dataset.csv\")\n",
    "\n",
    "# 2. Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"drying_time\"], errors=\"ignore\")\n",
    "y = df[\"drying_time\"]\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Scale features \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Configure and train MLP Regressor\n",
    "#    - Deeper architecture (512, 256, 128, 64, 32)\n",
    "#    - Lower alpha (0.01) for better balance\n",
    "#    - Increased max_iter=5000 for stability\n",
    "#    - learning_rate='adaptive'\n",
    "#    - early_stopping=True\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(512, 256, 128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.01,  # Regularization strength\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.0005,\n",
    "    max_iter=5000,  # Increased iterations\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 7. Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "accuracy = 100 - mape\n",
    "\n",
    "# 8. Save Metrics \n",
    "results = pd.DataFrame({\n",
    "    \"Version\": [\"Version 11: MLP (Batch Normalization + Increased Iterations)\"],\n",
    "    \"MSE\": [mse],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2\": [r2],\n",
    "    \"MAPE (%)\": [mape],\n",
    "    \"Accuracy (%)\": [accuracy]\n",
    "})\n",
    "results.to_csv(\"drying_time_model_comparison.csv\", mode=\"a\", index=False, header=False)\n",
    "\n",
    "# 9. Print results\n",
    "print(\"Version 11 (MLP - Batch Normalization + Increased Iterations) completed. Metrics saved.\")\n",
    "print(f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, \"\n",
    "      f\"R2: {r2:.5f}, MAPE: {mape:.5f}%, Accuracy: {accuracy:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERSION 12: MLP (Smaller Learning Rate + More Layers + Momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 12 (MLP - Smaller LR + More Layers + Momentum) completed. Metrics saved.\n",
      "MSE: 0.74768, RMSE: 0.86468, MAE: 0.63512, R2: 0.34478, MAPE: 12.26130%, Accuracy: 87.73870%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv(\"drying_time_dataset.csv\")\n",
    "\n",
    "# 2. Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"drying_time\"], errors=\"ignore\")\n",
    "y = df[\"drying_time\"]\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Scale features (Batch Normalization requires standardized input)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Configure and train MLP Regressor\n",
    "#    - More layers (1024, 512, 256, 128, 64, 32)\n",
    "#    - Smaller learning rate (0.0001) for finer weight updates\n",
    "#    - Momentum added via 'sgd' solver\n",
    "#    - Increased max_iter=7000 for better convergence\n",
    "#    - learning_rate='adaptive'\n",
    "#    - early_stopping=True\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(1024, 512, 256, 128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='sgd',  # Switch to stochastic gradient descent for momentum\n",
    "    alpha=0.01,  # Regularization strength\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.0001,  # Even smaller learning rate for finer updates\n",
    "    max_iter=7000,  # Increased iterations for deep training\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    momentum=0.9,  # Momentum term to accelerate learning\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 7. Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Ensure NumPy is used for sqrt\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "accuracy = 100 - mape\n",
    "\n",
    "# 8. Save Metrics \n",
    "results = pd.DataFrame({\n",
    "    \"Version\": [\"Version 12: MLP (Smaller LR + More Layers + Momentum)\"],\n",
    "    \"MSE\": [mse],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2\": [r2],\n",
    "    \"MAPE (%)\": [mape],\n",
    "    \"Accuracy (%)\": [accuracy]\n",
    "})\n",
    "results.to_csv(\"drying_time_model_comparison.csv\", mode=\"a\", index=False, header=False)\n",
    "\n",
    "# 9. Print results\n",
    "print(\"Version 12 (MLP - Smaller LR + More Layers + Momentum) completed. Metrics saved.\")\n",
    "print(f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, \"\n",
    "      f\"R2: {r2:.5f}, MAPE: {mape:.5f}%, Accuracy: {accuracy:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 13: MLP (LBFGS solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 13 MLP (LBFGS solver) completed. Metrics saved.\n",
      "MSE: 0.09957, RMSE: 0.31554, MAE: 0.14322, R2: 0.91275, MAPE: 2.39092%, Accuracy: 97.60908%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Load dataset\n",
    "df = pd.read_csv(\"drying_time_dataset.csv\")\n",
    "X = df.drop(columns=[\"drying_time\"], errors=\"ignore\")\n",
    "y = df[\"drying_time\"]\n",
    "\n",
    "# 2. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Configure MLP Regressor (LBFGS solver)\n",
    "#    - LBFGS is a quasi-Newton method that can yield good results on modest data sizes\n",
    "#    - alpha=0.002 for moderate regularization\n",
    "#    - hidden_layer_sizes=(300,200,100,50) for deeper capacity\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(300, 200, 100, 50),\n",
    "    activation='relu',\n",
    "    solver='lbfgs',      # LBFGS solver\n",
    "    alpha=0.002,\n",
    "    max_iter=2000,       # LBFGS can converge faster, but give it enough iterations\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 5. Fit the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 7. Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "accuracy = 100 - mape\n",
    "\n",
    "# 8. Save Results\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Version 13: MLP (LBFGS solver)\"],\n",
    "    \"MSE\": [mse],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2\": [r2],\n",
    "    \"MAPE (%)\": [mape],\n",
    "    \"Accuracy (%)\": [accuracy]\n",
    "})\n",
    "results.to_csv(\"drying_time_model_comparison.csv\", mode=\"a\", index=False, header=False)\n",
    "\n",
    "print(\"Version 13 MLP (LBFGS solver) completed. Metrics saved.\")\n",
    "print(f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, R2: {r2:.5f}, MAPE: {mape:.5f}%, Accuracy: {accuracy:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
